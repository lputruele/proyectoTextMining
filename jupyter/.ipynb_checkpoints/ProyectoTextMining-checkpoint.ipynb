{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8360b692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sys\n",
    "import re\n",
    "import numpy\n",
    "import pandas\n",
    "from nltk.util import ngrams\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.collocations import TrigramCollocationFinder\n",
    "from nltk.collocations import QuadgramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.metrics import TrigramAssocMeasures\n",
    "from nltk.metrics import QuadgramAssocMeasures\n",
    "from nltk.sentiment.util import extract_unigram_feats\n",
    "from nltk.sentiment.util import extract_bigram_feats\n",
    "from nltk.corpus import stopwords\n",
    "from pysentimiento.preprocessing import preprocess_tweet\n",
    "from pysentimiento import analyzer\n",
    "import json\n",
    "import networkx as nx\n",
    "import community as community_louvain\n",
    "import networkx.algorithms.community.kernighan_lin as community_alg\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc266d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(doc):\n",
    "    pdoc = preprocess_tweet(doc, shorten=2) #standarize retweets,urls,responses,laughs,emojis,etc\n",
    "    pdoc = pdoc.lower() #lowercase\n",
    "    pdoc = re.sub(r'[^\\w\\s]', '', pdoc) #remove punctuation\n",
    "    pdoc = ' '.join([w for w in pdoc.split() if w not in stopwords])\n",
    "    return pdoc\n",
    "\n",
    "def remove_duplicates(x):\n",
    "    return list(dict.fromkeys(x))\n",
    "\n",
    "def get_ngrams(words, top_n, min_freq, option, measure):\n",
    "    if (option==1):\n",
    "        fdist = nltk.FreqDist(words)\n",
    "        unigrams = []\n",
    "        for (w,d) in fdist.most_common(top_n):\n",
    "            unigrams.append(w)\n",
    "        print(\"top \"+str(top_n)+\" unigrams:\")\n",
    "        print(unigrams)\n",
    "        return unigrams\n",
    "    if (option==2):\n",
    "        bcf = BigramCollocationFinder.from_words(iter(words))\n",
    "        bcf.apply_freq_filter(min_freq)\n",
    "        bigrams = []\n",
    "        if (measure==\"pmi\"):\n",
    "            print (\"Using pmi assoc. measure...\")\n",
    "            bigrams = bcf.nbest(BigramAssocMeasures.pmi, top_n)\n",
    "        if (measure==\"chi\"):\n",
    "            print (\"Using chi-squared measure...\")\n",
    "            bigrams = bcf.nbest(BigramAssocMeasures.chi_sq, top_n)\n",
    "        if (measure==\"lr\"):\n",
    "            print (\"Using likelihood ratio assoc. measure...\")\n",
    "            bigrams = bcf.nbest(BigramAssocMeasures.likelihood_ratio, top_n)\n",
    "        if (measure==\"ps\"):\n",
    "            print (\"Using poisson stirling assoc. measure...\")\n",
    "            bigrams = bcf.nbest(BigramAssocMeasures.poisson_stirling, top_n)\n",
    "        if (measure==\"j\"):\n",
    "            print (\"Using jaccard assoc. measure...\")\n",
    "            bigrams = bcf.nbest(BigramAssocMeasures.jaccard, top_n)\n",
    "        if (measure==\"st\"):\n",
    "            print (\"Using student_t assoc. measure...\")\n",
    "            bigrams = bcf.nbest(BigramAssocMeasures.student_t, top_n)\n",
    "        print(\"top \"+str(top_n)+\" bigrams:\")\n",
    "        print(bigrams)\n",
    "        return bigrams\n",
    "    if (option==3):\n",
    "        tcf = TrigramCollocationFinder.from_words(iter(words))\n",
    "        tcf.apply_freq_filter(min_freq)\n",
    "        trigrams = tcf.nbest(TrigramAssocMeasures.likelihood_ratio, top_n)\n",
    "        print(\"top \"+str(top_n)+\" trigrams:\")\n",
    "        print(trigrams)\n",
    "        return trigrams\n",
    "    if (option==4):\n",
    "        qcf = QuadgramCollocationFinder.from_words(iter(words))\n",
    "        qcf.apply_freq_filter(min_freq)\n",
    "        quadgrams = qcf.nbest(QuadgramAssocMeasures.likelihood_ratio, top_n)\n",
    "        print(\"top \"+str(top_n)+\" quadgrams:\")\n",
    "        print(quadgrams)\n",
    "        return quadgrams\n",
    "\n",
    "def extract_ngram_feats(document, ngramss, option):\n",
    "    features = {}\n",
    "    for ngram in ngramss:\n",
    "        if option==1:\n",
    "            features['contains({0})'.format(ngram[0])] = ngram in document\n",
    "        if option==2:\n",
    "            features['contains({0} - {1})'.format(ngram[0], ngram[1])] = ngram in nltk.bigrams(document)\n",
    "        if option==3:\n",
    "            features['contains({0} - {1} - {2})'.format(ngram[0], ngram[1], ngram[2])] = ngram in nltk.trigrams(document)\n",
    "        if option==4:\n",
    "            features['contains({0} - {1} - {2} - {3})'.format(ngram[0], ngram[1], ngram[2], ngram[3])] = ngram in ngrams(document, 4)\n",
    "    return features\n",
    "\n",
    "def create_graph(words, option, top_n,measure):\n",
    "    # Extract ngram features\n",
    "    ngramss,matrix = create_matrix(words,option, top_n,measure)\n",
    "    G = nx.Graph()\n",
    "    for d in range(len(documents)):\n",
    "        G.add_node(d,tweet=documents[d], ptweet=documents_processed[d])\n",
    "    for d1 in range(len(documents)):\n",
    "        for d2 in range(len(documents)):\n",
    "            for ngram in ngramss:\n",
    "                feat_key = \"\"\n",
    "                if option==1:\n",
    "                    feat_key = \"contains(\"+ngram[0]+\")\"\n",
    "                if option==2:\n",
    "                    feat_key = \"contains(\"+ngram[0]+\" - \"+ngram[1]+\")\"\n",
    "                if option==3:\n",
    "                    feat_key = \"contains(\"+ngram[0]+\" - \"+ngram[1]+\" - \"+ngram[2]+\")\"\n",
    "                if option==4:\n",
    "                    feat_key = \"contains(\"+ngram[0]+\" - \"+ngram[1]+\" - \"+ngram[2]+\" - \"+ngram[3]+\")\"\n",
    "                if (matrix[d1][feat_key] and matrix[d2][feat_key]) and (d1!=d2):\n",
    "                    oldEdge = (d1,d2) if (d1,d2) in G.edges else None\n",
    "                    if oldEdge==None:\n",
    "                        G.add_edge(d1,d2,weight=1)\n",
    "                    else:\n",
    "                        G.adj[d1][d2]['weight'] += 1\n",
    "    return G\n",
    "\n",
    "def create_matrix(words,option,top_n,measure):\n",
    "    ngramss = get_ngrams(words,top_n,1,option,measure)\n",
    "    matrix = []\n",
    "    for d in documents_processed:\n",
    "        matrix.append(extract_ngram_feats(d.split(),ngramss,option))\n",
    "    return ngramss,matrix;\n",
    "\n",
    "\n",
    "def draw_graph(G,partition):\n",
    "    pos = nx.spring_layout(G)\n",
    "    #color the nodes according to their partition\n",
    "    cmap = cm.get_cmap('viridis', max(partition.values()) + 1)\n",
    "    nx.draw_networkx_nodes(G, pos, partition.keys(), node_size=10,cmap=cmap, node_color=list(partition.values()))\n",
    "    nx.draw_networkx_edges(G, pos, alpha=0.5)\n",
    "    nx.write_gexf(G, \"test.gexf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ea7b86",
   "metadata": {},
   "source": [
    "Abrimos el dataset de tweets, eliminamos tweets duplicados, esto nos deja 1607 tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "902fa8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('tweets.json',)\n",
    "data = json.load(f)\n",
    "documents = remove_duplicates([d['text'] for d in data])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d2e676",
   "metadata": {},
   "source": [
    "Adaptamos los stopwords tradicionales para que no incluyan palabras que puedan ayudar a detectar sentimiento, y que a su vez incluyan tags innecesarios.\n",
    "\n",
    "Luego preprocesamos los tweets, esto implica: \n",
    " *estandarizar retweets, urls, respuestas, risas y emojis\n",
    " *pasar todo a minuscula\n",
    " *eliminar signos de puntuación\n",
    " *eliminar stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9da716d2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cantidad de tweets (procesados):1607'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exceptions = ['no','pero','muy','sin','y','ni','contra','tanto','ellas','nosotras','nuestra','nuestras']\n",
    "stopwords = [x for x in stopwords.words('spanish') if x not in exceptions]\n",
    "stopwords.append('rt')\n",
    "stopwords.append('usuario')\n",
    "stopwords.append('url')\n",
    "stopwords.append('re')\n",
    "stopwords.append('emoji')\n",
    "documents_processed = [preprocess(d) for d in documents]\n",
    "\n",
    "\"Ejemplo de tweet original:\"+ documents[0]\n",
    "\"Ejemplo de tweet procesado:\"+documents_processed[0]\n",
    "\"Cantidad de tweets:\"+ str(len (documents))\n",
    "\"Cantidad de tweets (procesados):\"+ str(len (documents_processed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8984de43",
   "metadata": {},
   "source": [
    "Separamos tweets a favor y en contra (los primeros 785 tweets están etiquetados en contra, y el resto a favor). Esto nos sirve para luego validar los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed8c6b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_en_contra = documents[:785]\n",
    "tweets_a_favor = documents[785:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8c5b02",
   "metadata": {},
   "source": [
    "Tokenizamos todos los tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53421cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = ' '.join(documents_processed)\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "words = tokenizer.tokenize(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ac0a0e",
   "metadata": {},
   "source": [
    "Creamos un grafo donde los nodos son los tweets y los arcos representan n-gramas en común. Mientras más haya en común el arco se hace linearmente mas pesado. Aquí hay tres argumentos importantes:\n",
    "* n\n",
    "* número de n-gramas a considerar como features de acuerdo a la métrica de asociación\n",
    "* metrica de asociación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e9f0aae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using student_t assoc. measure...\n",
      "top 20 bigrams:\n",
      "[('aborto', 'legal'), ('no', 'aborto'), ('ala', 'vida'), ('si', 'ala'), ('vida', 'no'), ('banco', 'avilma'), ('salvemos', 'las2'), ('las2', 'vidas'), ('aborto', 'si'), ('y', 'gratuito'), ('corazón', 'verde'), ('sia', 'vida'), ('noal', 'aborto'), ('dorso', 'mano'), ('mano', 'índice'), ('educación', 'sexual'), ('hacia', 'abajo'), ('índice', 'hacia'), ('legal', 'seguro'), ('tono', 'piel')]\n",
      "ngramss:20\n",
      "dict_keys(['contains(aborto - legal)'])\n",
      "dict_keys(['contains(aborto - legal)'])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'contains(no - aborto)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-caf5f4a0e29f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"st\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#1:unigrams,#2:bigrams,3:trigrams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\"Cantidad de nodos:\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber_of_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\"Cantidad de arcos:\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber_of_edges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-e5f20a985b0c>\u001b[0m in \u001b[0;36mcreate_graph\u001b[0;34m(words, option, top_n, measure)\u001b[0m\n\u001b[1;32m     90\u001b[0m                     \u001b[0mfeat_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"contains(\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" - \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" - \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" - \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\")\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeat_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeat_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0md1\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0md2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m                     \u001b[0moldEdge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0md1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0md1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medges\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0moldEdge\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'contains(no - aborto)'"
     ]
    }
   ],
   "source": [
    "G = create_graph(words,2,20,\"st\") #1:unigrams,#2:bigrams,3:trigrams\n",
    "\"Cantidad de nodos:\" + str(G.number_of_nodes())\n",
    "\"Cantidad de arcos:\" + str(G.number_of_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad922594",
   "metadata": {},
   "source": [
    "Realizamos una partición del grafo utilizando el algoritmo de bisección de kernighan. Esto nos parte el grafo en exactamente 2 conjuntos.\n",
    "Luego evaluamos la precisión de cada partición respecto a los tweets que sabemos que estan a favor y en contra respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06e24f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = community_alg.kernighan_lin_bisection(G,max_iter=100,weight='weight')\n",
    "\n",
    "en_contra_p0 = 0\n",
    "a_favor_p0 = 0\n",
    "en_contra_p1 = 0\n",
    "a_favor_p1= 0\n",
    "\n",
    "for d in partition[0]:\n",
    "    if (documents[d] in tweets_en_contra):\n",
    "        en_contra_p0 += 1\n",
    "    if (documents[d] in tweets_a_favor):\n",
    "        a_favor_p0 += 1\n",
    "for d in partition[1]:\n",
    "    if (documents[d] in tweets_en_contra):\n",
    "        en_contra_p1 += 1\n",
    "    if (documents[d] in tweets_a_favor):\n",
    "        a_favor_p1 += 1\n",
    "    \n",
    "print(\"particion 0:\")\n",
    "print(\"  tweets a favor:\" + str(a_favor_p0) + \"(\"+str((a_favor_p0/(a_favor_p0+en_contra_p0))*100)+\"%)\")\n",
    "print(\"  tweets en contra:\" + str(en_contra_p0) + \"(\"+str((en_contra_p0/(a_favor_p0+en_contra_p0))*100)+\"%)\")\n",
    "print(\"particion 1:\")\n",
    "print(\"  tweets a favor:\" + str(a_favor_p1) + \"(\"+str((a_favor_p1/(a_favor_p1+en_contra_p1))*100)+\"%)\")\n",
    "print(\"  tweets en contra:\" + str(en_contra_p1) + \"(\"+str((en_contra_p1/(a_favor_p1+en_contra_p1))*100)+\"%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
